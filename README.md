1)Implement a Perceptron from Scratch
Implement a single-layer perceptron using NumPy.
Train it on linearly separable data (e.g., AND/OR gate or
synthetic dataset).
Visualize decision boundary.


2)Build and Train a Feedforward Neural Network
Use PyTorch or TensorFlow to build an MLP.
Train on MNIST or Fashion-MNIST dataset.
Tune hidden layers and activation functions.
Report accuracy and loss curves.


3)Experiment with Regularization Techniques
Train a model with and without L2 regularization.
Add Dropout and compare performance.
Visualize training/validation loss and accuracy.


4)Visualize and Compare Activation Functions
Train the same MLP using Sigmoid, Tanh, and ReLU.
Plot training speed, accuracy, and gradient flow.
Discuss vanishing/exploding gradient issues.

5)Build a CNN for Image Classification
Build a simple CNN (e.g., 2 conv layers + FC) for CIFAR-10
or MNIST.
Compare with MLP performance.
Use basic data augmentation

6)Implement an RNN or LSTM for Text Classification
Use IMDB sentiment classification dataset.
Implement an LSTM for binary classification.
Plot accuracy and visualize example predictions.

7)Transfer Learning with a Pretrained CNN
Load a pretrained model (e.g., ResNet50 or VGG16).
Fine-tune it on a small custom dataset (e.g., cats vs dogs).
Freeze layers and experiment with performance.

8)Hyperparameter Tuning and Model Comparison
Choose any dataset and architecture.
Experiment with learning rate, batch size, number of layers,
etc.
Use tools like TensorBoard or WandB to log results.
Write a short report comparing configurations.
