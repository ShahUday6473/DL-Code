1)Implement a Perceptron from Scratch
o Implement a single-layer perceptron using NumPy.
o Train it on linearly separable data (e.g., AND/OR gate or
synthetic dataset).
o Visualize decision boundary.


2)Build and Train a Feedforward Neural Network
o Use PyTorch or TensorFlow to build an MLP.
o Train on MNIST or Fashion-MNIST dataset.
o Tune hidden layers and activation functions.
o Report accuracy and loss curves.


3)Experiment with Regularization Techniques
o Train a model with and without L2 regularization.
o Add Dropout and compare performance.
o Visualize training/validation loss and accuracy.


4)Visualize and Compare Activation Functions
o Train the same MLP using Sigmoid, Tanh, and ReLU.
o Plot training speed, accuracy, and gradient flow.
o Discuss vanishing/exploding gradient issues.

5)Build a CNN for Image Classification
o Build a simple CNN (e.g., 2 conv layers + FC) for CIFAR-10
or MNIST.
o Compare with MLP performance.
o Use basic data augmentation

6)Implement an RNN or LSTM for Text Classification
o Use IMDB sentiment classification dataset.
o Implement an LSTM for binary classification.
o Plot accuracy and visualize example predictions.

7)Transfer Learning with a Pretrained CNN
o Load a pretrained model (e.g., ResNet50 or VGG16).
o Fine-tune it on a small custom dataset (e.g., cats vs dogs).
o Freeze layers and experiment with performance.

8)Hyperparameter Tuning and Model Comparison
o Choose any dataset and architecture.
o Experiment with learning rate, batch size, number of layers,
etc.
o Use tools like TensorBoard or WandB to log results.
o Write a short report comparing configurations.
